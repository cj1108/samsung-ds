{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setting\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "## [문제 1] 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/linear_regression.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset distribution    \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P1.1] 경사 하강법을 이용해 최적의 선형 회귀 모델을 찾아주세요. 단, <font color=red>Tensorflow</font>는 사용하지 마세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적의 모델이란, 모든 데이터에 대해서 실제값과 예측값의 차이가 제일 작은 모델을 말합니다.\n",
    "### 선형 회귀 모델은  일반적으로 <font color=red>MSE(Mean Squared Error)</font> 손실을 최소화하도록 학습해야합니다.\n",
    "### 손실 함수와 모델 파라미터의 gradient에 관한 빈칸 부분을 채워넣어주세요.\n",
    "\n",
    "**MSE 손실 함수**\n",
    "\\begin{equation*}\n",
    "\\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))^2 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients 계산**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b)) * X_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial b} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-7\n",
    "\n",
    "# Model weights and bias parameters\n",
    "# f(x) = w * X + b\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Store model parameters and loss for visualization\n",
    "w_list, b_list, loss_list = [], [], []\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "######## Hint: use +, -, *, /, np.mean() ########\n",
    "    # MSE loss\n",
    "    # a**2 = a * a = square of a, a**3 = a * a * a\n",
    "    loss = \n",
    "    \n",
    "    # derivative w.r.t to w\n",
    "    dw = \n",
    "    # derivative w.r.t to b\n",
    "    db =\n",
    "#################################################\n",
    "    \n",
    "    # update w and b\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    w_list.append(w)\n",
    "    b_list.append(b)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과를 그려보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w * X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the intermediate trained model\n",
    "epochs_list = [1, 200, 400, 600, 800, 1000]\n",
    "\n",
    "for i in range(len(epochs_list)):\n",
    "    plt.scatter(X, y)   # scatter the original data\n",
    "    \n",
    "    # Load trained weights in specific epoch\n",
    "    epoch = epochs_list[i] - 1   # In python, all indexes start from 0\n",
    "    w = w_list[epoch]\n",
    "    b = b_list[epoch]\n",
    "    \n",
    "    y_pred = w * X + b\n",
    "    plt.plot(X, y_pred, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the change of loss\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [P1.2] 이번엔 <font color=red>Tensorflow</font>를 활용해, 경사 하강법으로 최적의 선형 회귀 모델을 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-7\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "\n",
    "    # Define MSE loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        loss = \n",
    "        \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    dw, db =    # dloss_dw, dloss_db\n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w.\n",
    "    b.\n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w = w.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과를 그려보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w * X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 모델의 파라미터가 정확히 일치합니다.\n",
    "### 이처럼 Tensorflow를 사용하시면, 직접 파라미터의 gradient 값을 계산할 필요가 없어 매우 편리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제 2] 비선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제로는 선형적으론 표현하기 힘든 복잡한 데이터들이 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/polynomial_regression.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset distribution    \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [P2.1] <font color=red>Tensorflow</font>를 통한 경사 하강법으로, 최적의 비선형 회귀 모델을 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-7\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "############# Hint : Same as before #############\n",
    "\n",
    "    # Define MSE loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        loss = \n",
    "    \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    dw, db =    # dloss_dw, dloss_db\n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w.\n",
    "    b.\n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w = w.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과를 그려보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w * X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 회귀 모델의 경우, 위의 데이터를 잘 표현하지 못하는 것 같습니다.\n",
    "### 이럴 경우, 좀 더 복잡한 회귀 모델을 이용해 데이터를 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P2.2] 이번엔 <font color=red>Tensorflow</font>를 이용한 경사 하강법으로, 최적의 3차 회귀 모델을 찾아주세요.\n",
    "\n",
    "### Cubic regression model. (3차 함수)\n",
    "\\begin{equation*}\n",
    "f(x) = w_1X^3 + w_2X^2 + w_3X + b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-14\n",
    "\n",
    "\n",
    "#################################################\n",
    "### Define model weights and bias parameters ###\n",
    "w1 = \n",
    "w2 = \n",
    "w3 = \n",
    "b = \n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "####### Hint : Consider multiple weights  #######\n",
    "\n",
    "    # Define MSE loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        loss = \n",
    "    \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    # dloss_dw1, dloss_dw2, dloss_dw3, dloss_db\n",
    "    dw1, dw2, dw3, db = \n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w1.\n",
    "    w2.\n",
    "    w3.\n",
    "    b.\n",
    "    \n",
    "#################################################\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w1 = w1.numpy()\n",
    "w2 = w2.numpy()\n",
    "w3 = w3.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "# Print the trained parameters value\n",
    "print('Trained model weights 1 : %.4f' % w1)\n",
    "print('Trained model weights 2 : %.4f' % w2)\n",
    "print('Trained model weights 3 : %.4f' % w3)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과를 그려보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = w1*X**3 + w2*X**2 + w3*X + b\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제 3] Logistic 회귀 (분류 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/logistic_regression.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.head()   # show the 5 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### '나이' 정보에 근거해 유저가 구매를 할지, 안할지를 구별하는 logistic 회귀 모델을 학습할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'Age' and 'Purchased' data\n",
    "X = data['Age'].to_numpy(dtype=np.float32)\n",
    "y = data['Purchased'].to_numpy()\n",
    "\n",
    "# Normalize 'Age' value\n",
    "# X의 평균값이 0이 될 수 있도록 만들어, 학습의 안정성을 높이는 방법.\n",
    "def normalize(X):\n",
    "    return X - X.mean()\n",
    "X = normalize(X)\n",
    "\n",
    "# Visualizing the dataset\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보시는 바와 같이 어릴수록 구매를 하지 않고, 나이가 많을수록 구매를 하는 경향성을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P3.1] <font color=red>Tensorflow</font>를 이용하지 않고, 최적의 logistic 회귀 모델을 경사 하강법으로 찾아주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logitstic 함수의 모양은 S 모양으로, 0부터 1 사이의 값으로 변환시켜줍니다.\n",
    "### 따라서 확률의 관점에서, Logistic 회귀 모델의 output이 0.5보다 크냐, 작냐를 기준으로 데이터를 분류할 수 있습니다.\n",
    "### Logistic 회귀 모델은 -가능도 손실 함수를 최소화하는 방향으로 학습합니다.\n",
    "\n",
    "**Logistic 회귀 모델**\n",
    "\\begin{equation*}\n",
    "P(y_i=1|X_i) = \\frac{1}{1 + e^{-(wX_i + b)}}\n",
    "\\end{equation*}\n",
    "\n",
    "**-가능도 손실 함수**\n",
    "\\begin{equation*}\n",
    "-\\frac{1}{n} \\sum_{i=1}^n (y_i * log(P(y_i=1|X_i)) + (1 - y_i) * log(1 - P(y_i=1|X_i)))\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients 계산**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n ((P(y_i=1|X_i) - y_i) * X_i)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (P(y_i=1|X_i) - y_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "######## Hint: use +, -, *, /, **, np.mean(), #######\n",
    "########           np.exp(), np.log() ###########\n",
    "    y_pred = 1 ? (1 ? ???(-(w ? X ? b)))  # Logistic regression\n",
    "    loss =   # Likelihood loss\n",
    "    \n",
    "    dw =   # gradients w.r.t to w\n",
    "    db =   # gradients w.r.t to b\n",
    "    \n",
    "    # Update model weights and bias\n",
    "    w = \n",
    "    b = \n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과를 그려보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = 1 / (1 + np.exp(-(w*X + b)))\n",
    "plt.scatter(X, y_pred, color='red')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--')   # show 0.5 threshold line\n",
    "\n",
    "# Accurate results are colored as orange\n",
    "index1 = (y == 0) * (y_pred < 0.5)\n",
    "index2 = (y == 1) * (y_pred > 0.5)\n",
    "index = index1 + index2\n",
    "plt.scatter(X[index], y[index], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [P3.2] <font color=red>Tensorflow</font>를 이용한 경사 하강법을 통해 최적의 logistic 회귀 모델을 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "\n",
    "    # Define Likelihood loss function (Hint: tf.GradientTape(), tf.reduce_mean())\n",
    "    with ??? as tape:\n",
    "        # Hint: use tf.exp()\n",
    "        y_pred = 1 ? (1 ? ???(-(w ? X ? b)))\n",
    "        # Hint: use tf.math.log\n",
    "        loss = \n",
    "    \n",
    "    \n",
    "    # Get gradients of parameters (Hint: tape.gradient())\n",
    "    dw, db =    # dloss_dw, dloss_db\n",
    "    \n",
    "    \n",
    "    # Update model weights and bias (Hint : assign_sub())\n",
    "    w.\n",
    "    b.\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "# Convert parameters type from tensor to numpy\n",
    "w = w.numpy()\n",
    "b = b.numpy()\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과를 그려보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = 1 / (1 + tf.exp(-(w*X + b)))\n",
    "plt.scatter(X, y_pred, color='red')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--')   # show 0.5 threshold line\n",
    "\n",
    "# Accurate results are colored as orange\n",
    "index1 = (y == 0) * (y_pred.numpy() < 0.5)\n",
    "index2 = (y == 1) * (y_pred.numpy() > 0.5)\n",
    "index = index1 + index2\n",
    "plt.scatter(X[index], y[index], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
